# MU-ALIGN: Unlearning Through Alignment

<div align="center">

[![Paper](https://img.shields.io/badge/Paper-USENIX%20Submission-red.svg)](https://arxiv.org)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.8+-green.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)

**Official Implementation of "MU-ALIGN: Machine Unlearning through Decision-Space Alignment"**

*Submitted to USENIX Security 2026*

[Paper](https://arxiv.org) | [Documentation](docs/) | [Results](results/)

</div>

---

## ğŸ“‹ Overview

**MU-ALIGN** is a novel machine unlearning framework that achieves privacy-preserving model updates through **decision-space alignment**. Unlike existing methods that focus on representation-space modifications, MU-ALIGN directly enforces uniform output distributions over forget samples, achieving:

- ğŸ”’ **Superior Privacy**: Near-zero TPR (<0.1%) at extreme low FPR (10â»âµ) against membership inference attacks
- âš¡ **Computational Efficiency**: 2.4Ã— faster than retraining on VQA-v2 benchmarks  
- ğŸ¯ **Preservation of Utility**: Maintains >98% retention accuracy while achieving complete forgetting
- ğŸ›¡ï¸ **Robustness**: Effective against state-of-the-art attacks (SCRUB, LiRA, confidence-based MI)

### Key Contributions

1. **Novel Alignment Objective**: Decision-space uniformity enforcement via KL divergence minimization
2. **Tail Suppression**: Explicit mechanism to eliminate high-confidence outliers that enable membership inference
3. **Comprehensive Evaluation**: First work to evaluate unlearning at extreme low-FPR regime (10â»âµ) across vision-language models
4. **Scalability**: Demonstrated effectiveness on VQA-v2 datasets ranging from 5k to 80k samples

---

## ğŸš€ Quick Start

### Prerequisites

```bash
Python >= 3.8
PyTorch >= 2.0
CUDA >= 11.7 (for GPU acceleration)
```

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/mu-align.git
cd mu-align

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install package
pip install -e .
```

### Basic Usage

```python
from mu_align import MUAlign
from mu_align.utils import load_model, load_data

# Load pretrained model and data
model = load_model('vilt-vqa', checkpoint='path/to/checkpoint.pth')
forget_data, retain_data = load_data('vqa_v2', split='20k')

# Initialize MU-ALIGN
unlearner = MUAlign(
    model=model,
    alpha=1.0,          # Uniformity loss weight
    beta=0.5,           # Tail suppression weight
    temperature=2.0     # Temperature for KL divergence
)

# Perform unlearning
unlearned_model = unlearner.unlearn(
    forget_loader=forget_data,
    retain_loader=retain_data,
    epochs=5
)

# Evaluate privacy
from mu_align.attacks import evaluate_privacy

results = evaluate_privacy(
    model=unlearned_model,
    forget_data=forget_data,
    retain_data=retain_data,
    attacks=['confidence', 'scrub', 'lira']
)

print(f"TPR @ FPR=10â»âµ: {results['tpr_1e5']:.4f}")
```

---

## ğŸ“ Repository Structure

```
mu-align/
â”œâ”€â”€ attacks/                    # Membership inference attacks
â”‚   â”œâ”€â”€ confidence_attack.py   # Confidence-based MI
â”‚   â”œâ”€â”€ scrub_attack.py        # SCRUB evaluation
â”‚   â””â”€â”€ lira_attack.py         # LiRA implementation
â”‚
â”œâ”€â”€ baselines/                  # Baseline unlearning methods
â”‚   â”œâ”€â”€ retrain.py             # Retraining from scratch
â”‚   â”œâ”€â”€ scrub.py               # SCRUB unlearning
â”‚   â”œâ”€â”€ fisher.py              # Fisher forgetting
â”‚   â”œâ”€â”€ amnesiac.py            # Amnesiac unlearning
â”‚   â””â”€â”€ multidelete.py         # Multiple deletion
â”‚
â”œâ”€â”€ experiments/                # Experiment configurations
â”‚   â”œâ”€â”€ vqa_v2_5k.yaml         # VQA-v2 5k split
â”‚   â”œâ”€â”€ vqa_v2_20k.yaml        # VQA-v2 20k split
â”‚   â””â”€â”€ vqa_v2_full.yaml       # VQA-v2 full dataset
â”‚
â”œâ”€â”€ models/                     # Model architectures
â”‚   â”œâ”€â”€ vilt.py                # ViLT vision-language model
â”‚   â””â”€â”€ mu_align.py            # MU-ALIGN core implementation
â”‚
â”œâ”€â”€ plots/                      # Visualization scripts
â”‚   â”œâ”€â”€ plot_privacy.py        # Privacy analysis plots
â”‚   â”œâ”€â”€ plot_utility.py        # Utility preservation plots
â”‚   â””â”€â”€ plot_distribution.py   # Score distribution analysis
â”‚
â”œâ”€â”€ results/                    # Experimental results
â”‚   â”œâ”€â”€ privacy_cache/         # Cached evaluation results
â”‚   â””â”€â”€ figures/               # Generated figures
â”‚
â”œâ”€â”€ main.py                     # Main training script
â”œâ”€â”€ main.ipynb                  # Jupyter notebook demo
â”œâ”€â”€ agg.py                      # Results aggregation
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

---

## ğŸ”¬ Experiments

### Reproducing Paper Results

#### 1. VQA-v2 20k Experiment (Table 2)

```bash
# Run MU-ALIGN
python main.py \
    --config experiments/vqa_v2_20k.yaml \
    --method mu_align \
    --alpha 1.0 \
    --beta 0.5 \
    --seed 5508

# Run baselines
for method in scrub retr fisher amnesiac multidelete; do
    python main.py \
        --config experiments/vqa_v2_20k.yaml \
        --method $method \
        --seed 5508
done

# Aggregate results
python agg.py --experiment vqa_v2_20k --output results/table2.csv
```

#### 2. Privacy Evaluation (Figure 4, Table 9)

```bash
# Evaluate membership inference at fixed FPR
python attacks/evaluate_privacy.py \
    --models results/vqa_v2_20k/*.pth \
    --fprs 1e-5 1e-4 5e-4 1e-3 \
    --attacks confidence scrub lira \
    --output results/privacy_eval.json

# Generate diagnostic plots
python plots/plot_distribution.py \
    --results results/privacy_eval.json \
    --output results/figures/figure4.pdf
```

#### 3. Ablation Studies (Table 6)

```bash
# Vary alpha (uniformity weight)
for alpha in 0.0 0.5 1.0 2.0; do
    python main.py \
        --config experiments/vqa_v2_20k.yaml \
        --method mu_align \
        --alpha $alpha \
        --beta 0.5 \
        --tag ablation_alpha_$alpha
done

# Vary beta (tail suppression weight)
for beta in 0.0 0.25 0.5 1.0; do
    python main.py \
        --config experiments/vqa_v2_20k.yaml \
        --method mu_align \
        --alpha 1.0 \
        --beta $beta \
        --tag ablation_beta_$beta
done
```

---

## ğŸ“Š Results

### Privacy vs Utility Trade-off

| Method       | Retain Acc | Forget Acc | TPR@10â»âµ | TPR@10â»Â³ |
|--------------|------------|------------|----------|----------|
| **Original** | 0.38       | 0.14       | 6.0Ã—10â»â´ | 2.5Ã—10â»Â² |
| SCRUB        | 0.39       | 0.00       | **0.9821** | **0.9912** |
| Retraining   | 0.38       | 0.00       | 2.3Ã—10â»Â³ | 1.6Ã—10â»Â² |
| **MU-ALIGN** | **0.39**   | **0.00**   | **6.0Ã—10â»â´** | **4.8Ã—10â»Â³** |

*Table: Main results on VQA-v2 20k split. MU-ALIGN achieves lowest TPR while maintaining utility.*

### Key Findings

âœ… **Near-Perfect Forgetting**: 0.0% accuracy on forget set (vs 14% original)  
âœ… **Extreme Low-FPR Privacy**: TPR < 0.1% at FPR = 10â»âµ  
âœ… **Utility Preservation**: 98% retain accuracy maintained  
âœ… **Computational Efficiency**: 2.4Ã— faster than retraining

See [RESULTS.md](RESULTS.md) for detailed analysis and additional experiments.

---

## ğŸ” Method Details

### MU-ALIGN Objective

The core unlearning objective combines three components:

```
L_total = L_retain + Î±Â·L_uniform + Î²Â·L_tail

where:
  L_retain  = Cross-entropy loss on retain set (utility preservation)
  L_uniform = KL(p_Î¸(y|x) || Uniform) on forget set (alignment)
  L_tail    = Max-suppression penalty on top-k confidences (tail suppression)
```

### Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `alpha` | 1.0 | Uniformity loss weight |
| `beta` | 0.5 | Tail suppression weight |
| `temperature` | 2.0 | KL divergence temperature |
| `top_k` | 10 | Number of top predictions to suppress |
| `lr` | 1e-5 | Learning rate |
| `epochs` | 5 | Unlearning epochs |

---

## ğŸ› ï¸ Advanced Usage

### Custom Datasets

```python
from mu_align.data import UnlearningDataset

# Create custom forget/retain split
dataset = UnlearningDataset(
    data_path='path/to/data',
    forget_indices=forget_idx,
    retain_indices=retain_idx,
    transform=transforms
)

forget_loader = DataLoader(dataset.forget, batch_size=32)
retain_loader = DataLoader(dataset.retain, batch_size=32)
```

### Custom Attacks

```python
from mu_align.attacks import BaseAttack

class MyCustomAttack(BaseAttack):
    def compute_scores(self, model, data):
        # Your attack logic here
        return member_scores, non_member_scores
    
# Evaluate
attack = MyCustomAttack()
results = attack.evaluate(model, forget_data, retain_data)
```

### Distributed Training

```bash
# Multi-GPU training
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    main.py \
    --config experiments/vqa_v2_full.yaml \
    --distributed
```

---

## ğŸ“ˆ Visualization

### Generate All Plots

```bash
# Privacy analysis
python plots/plot_privacy.py --results results/ --output figures/

# Utility analysis  
python plots/plot_utility.py --results results/ --output figures/

# Score distributions
python plots/plot_distribution.py --results results/ --output figures/

# Ablation studies
python plots/plot_ablation.py --results results/ablations/ --output figures/
```

---

## ğŸ¤ Citation

If you use this code or find our work helpful, please cite:

```bibtex
@inproceedings{mu-align2026,
  title={MU-ALIGN: Machine Unlearning through Decision-Space Alignment},
  author={[Your Name] and [Co-authors]},
  booktitle={USENIX Security Symposium},
  year={2026}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Datasets**: [VQA v2.0](https://visualqa.org/) 
- **Base Models**: [ViLT](https://github.com/dandelin/vilt) (Vision-and-Language Transformer)
- **Baselines**: Implementation adapted from [SCRUB](https://github.com/facebookresearch/SCRUB)

---

## ğŸ“ Contact

For questions or issues, please:
- Open an issue on [GitHub Issues](https://github.com/yourusername/mu-align/issues)


---

## ğŸ”„ Updates

**Latest Release**: v1.0.0 (February 2026)

- âœ… Initial release with VQA-v2 experiments
- âœ… Support for 5 baseline methods
- âœ… Comprehensive privacy evaluation suite
- âœ… Distributed training support

See [CHANGELOG.md](CHANGELOG.md) for detailed version history.

---

<div align="center">

**â­ Star this repository if you find it helpful!**

Made with â¤ï¸ for the research community

</div>
